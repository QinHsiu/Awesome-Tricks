- 梯度下降算法
  - 全量梯度下降，也即对所有的训练数据计算损失并进行梯度反向传播，这样会导致计算开销大，从而使得模型的推理速度变慢很多
  - 小批量随机梯度下降，对每一个小批量内的数据计算损失并进行梯度反向传播，这样可以使用矩阵计算加速并行，引入随机性可以避免困在局部最优解
- 常用的优化更新策略
  - 动量更新
- 其他资源
    - [梯度下降算法](https://mp.weixin.qq.com/s/dmwMPnOsr15eFTQOpmf8Og)
    - [梯度下降算法](https://mp.weixin.qq.com/s/48vMmq-3Js374sh0HpAP0A)
    - [优化算法相关理论](https://mp.weixin.qq.com/s/pj_Vs_S5Lkc0h3qXOv2mYQ)
    - [优化算法讲解](https://mp.weixin.qq.com/s/AEb5a0jct-5v8w2kV4iQXg)
    - [常用的优化算法-机器学习](https://mp.weixin.qq.com/s/G6wzoPZL_hflSVr3dWY-Cg)
    - [常用优化算法-深度学习](https://mp.weixin.qq.com/s/3dtsdxNuQD4FD4gRQpN8fA)
    - [优化算法小结](https://mp.weixin.qq.com/s/kj8VulNMfEgVShnyE43G9A)
    - [优化器与学习率设置详解](https://mp.weixin.qq.com/s/dGbGc3KHITVLT-uCFe5_vw)
